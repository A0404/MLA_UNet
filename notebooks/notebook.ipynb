{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les réseaux convolutionnels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réseau inspiré de http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et conditionnement des données d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement et normalisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(2),         # Ajoute 2 pixels de padding sur chaque bord\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "training_dataset = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convention pour les images en entrée dans les réseaux de neurones convolutifs en PyTorch est d'utiliser des tenseurs de taille `(batch_size, channels, height, width)`. \n",
    "\n",
    "- `batch_size` correspond au nombre d'images dans un batch\n",
    "- `channels` correspond au nombre de canaux de couleur de l'image. Pour une image en niveaux de gris, `channels` est égal à 1. Pour une image en couleur, `channels` est égal à 3 (rouge, vert, bleu)\n",
    "- `height` correspond à la hauteur de l'image\n",
    "- `width` correspond à la largeur "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Architecture du réseau "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concevoir un réseau inspiré LeNet5 http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\n",
    "![LeNet5](https://miro.medium.com/max/2000/1*1TI1aGBZ4dybR6__DI9dzA.png)\n",
    "\n",
    "\n",
    "La structure globale sera conservée (nombre de couches, nombre de neurones par couche…) mais dans cette version :\n",
    "- Les couches de convolution sont totalement connectées\n",
    "- La sortie est une couche fully connected (FC) classique \n",
    "- La fonction de coût est l’entropie croisée\n",
    "\n",
    "\n",
    "Vous utiliserez la `nn.Sequential` pour le backbone et vous ajouterez une dernière couche pour la partie classlification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(in_features=120 * 1 * 1, out_features=84)\n",
    "        self.fc2 = nn.Linear(in_features=84, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('Input:', x.shape[1:])\n",
    "        x = self.relu(self.conv1(x))\n",
    "        #print('Après conv1:', x.shape[1:])\n",
    "        x = self.pool(x)\n",
    "        #print('Après pool1:', x.shape[1:])\n",
    "        x = self.relu(self.conv2(x))\n",
    "        #print('Après conv2:', x.shape[1:])\n",
    "        x = self.pool(x)\n",
    "        #print('Après pool2:', x.shape[1:])\n",
    "        x = self.relu(self.conv3(x))\n",
    "        #print('Après conv3:', x.shape[1:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print('Après flatten:', x.shape[1:])\n",
    "        x = self.dropout(self.relu(self.fc1(x)))  # Dropout ajouté ici\n",
    "        #x = self.relu(self.fc1(x))\n",
    "        #print('Après fc1:', x.shape[1:])\n",
    "        x = self.fc2(x)\n",
    "        #print('Après fc2:', x.shape[1:])\n",
    "        return x\n",
    "\n",
    "mnist_model = MNISTModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apprentissage du réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des paramètres de l'optimisation (fonction de coût, méthode d'optimisation, hyperparamètres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "sgd = optim.SGD(mnist_model.parameters(), lr=0.01)\n",
    "num_workers = max(1, min(8, os.cpu_count() // 2))\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=64, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boucle d'apprentissage (faire 10 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adrie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 1.905048131942749 (last batch)\n",
      "epoch 2: 0.2816218137741089 (last batch)\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    mnist_model.train()  # mode entraînement\n",
    "    for data, target in train_loader:\n",
    "        sgd.zero_grad()\n",
    "        outputs = mnist_model(data)\n",
    "        # converti automatiquement les labels dans targets en vecteur one hot\n",
    "        loss = criterion(outputs, target)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        sgd.step()\n",
    "\n",
    "    #loss calculé uniquement sur le dernier batch de l'epoch\n",
    "    print('epoch {}: {} (last batch)'.format(epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation du modèle sur la base de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On visualise le resultat pour `b_size` images et on les compare à la vérité terrain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 1, 0, 4])\n",
      "tensor([7, 2, 1, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "#data, target = next(iter(test_loader))\n",
    "b_size = 5\n",
    "data, target = next(iter(test_loader))\n",
    "data = data[:b_size]\n",
    "target = target[:b_size]\n",
    "pred = mnist_model(data)\n",
    "print(pred.argmax(dim=1, keepdim=False))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On évalue les performances sur l'ensemble des données d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0017, Accuracy: 9660/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# pour ne aps calculer les gradients (gain de temps et de mémoire)\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = mnist_model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation des filtres appris par le réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ecrire une fonction de visualisation des filtres avec la signature suivante :  \n",
    "`displayConvFilers(model, layer_name, num_filter=4, filter_size=(3,3), num_channel=0, fig_size=(2,2))`\n",
    "- `model` : le réseau de neurones dont on souhaite visualiser les filtres\n",
    "- `layer_name` : le nom de la couche dont on souhaite visualiser les filtres \n",
    "- `num_channel` : le numéro du canal dont on souhaite visualiser les filtres\n",
    "- `fig_size` : les dimensions de la figure. Doivent être en cohérence avec le nombre de filtres de la couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'conv3.weight', 'conv3.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#afficher la liste des noms des couches\n",
    "filters = mnist_model.state_dict()\n",
    "print(filters.keys())\n",
    "\n",
    "#Pour récupérer les poids de la première couche\n",
    "filters  = mnist_model.state_dict()['conv1.weight']\n",
    "filters.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTModel(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Ces noms peuvent se déduire de l'affichage de la structure du réseau\n",
    "print(mnist_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayConvFilters(model, layer_name, num_filter=4, filter_size=(3,3), num_channel=0, fig_size=(2,2)):\n",
    "    # Récupère les poids de la couche demandée\n",
    "    filters = getattr(model, layer_name).weight.data.cpu().numpy()\n",
    "    plt.figure(figsize=fig_size)\n",
    "    for i in range(num_filter):\n",
    "        plt.subplot(fig_size[0], fig_size[1], i+1)\n",
    "        # Affiche le filtre du canal demandé\n",
    "        plt.imshow(filters[i, num_channel, :filter_size[0], :filter_size[1]], cmap='gray')\n",
    "        plt.title(f'Filtre {i}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appeler deux fois la fonction pour afficher les filtres de la première couche et le premier canal des filtres de la deuxième couche de convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtres/Canaux de la 1ière couche de convolution\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x300 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ier Filtre/Canal de la 2e couche de convolution\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEwAAABZCAYAAACUjMIDAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAABLlJREFUeJztnF0ou18cwL/7j0ZDfrgYJWmIuFBjUmsXu8SF5P2GkMSNsvJS8pK5UW6kpLSJXHm7oyiXWi68ReRii6SMQjQuOL/OqS2bl53v/Lffr37fTw07zzlnz/N5vuec7549o2CMMSCk+U++KkHCgoCEISFhSEgYEhKGhIQhIWFISNifFOZ0OkGhUIDNZvOWDQ4OirJ/UpjNZhMH/9mjp6dHup/R0VFYXV2FcPLy8gLd3d2QkpIC0dHRUFRUBBsbG+h+IoJ58eHhYUhPT/cpy8vLg7S0NHC73RAZGRlQWGVlJZSXl0O4aGxshMXFRejs7ITMzExx8ktKSmBrawsMBoN8RwyB1Wrlb9TZzs6OdJuBgQHR5j1qtZo1NDRItX98fGQ/xW63i30YGxvzlrndbqbVallxcTGqr5DPYf7w7U9PTzA7O+sdzvzsv5/vjo+Pob6+Hn79+uVz9ufn50Gn04khlZCQALW1tXBxcQGB4JGlVCqhtbXVWxYVFQXNzc2wvb0t1cePhuT9/T3c3Nz4lCUlJUm1nZubg5aWFtDr9d4D0Gq1PnWqqqrEsOFD13P1yWKxQH9/P1RXV4v2LpcLJiYmwGg0wu7uLsTHx3/5mnx7VlYWxMXF+ZTzfeDs7e1Bamqq3MEHMyThkwfH4XCIv3m9YIakp25dXZ1PudPpZEqlklksFp/yw8NDFhER8aHcn9zcXGYymT6UHx0didebmpqSNMBYUBE2OTkpzlioaGtr83m+vLwMb29vIrreR7ZGoxGRyCfuvr6+L/vjC5FKpfpQzoelZ7ssQQnT6/VQUFAAocJ/BT47OxNDk8v5jECrMp/zeFrhz/Pzs3d7SIWFGv8D4NHFF4O1tTUxefsTExPzbX/JyclweXn5ofzq6kr85rnZXy1Mgcz8+aLAI4xHXjBTQX5+vhi2Dw8PPhO/3W73bv+r30uq1Wq4u7uTrl9RUSEia2hoyLtqeuDPb29vv23Pk+TX11eYnp72lvEharVaRcYvvUL+qQjT6XSwubkJ4+PjYjjwyOE7/l2EjYyMQG9vr8j1+DuE2NhYcDgcsLKyItITs9n8ZXveN09VePvr62vIyMgQeSDva2ZmBrfz/2em75BMK05OTpjRaGTR0dFimyfF8NR1uVyf9r+0tMQMBoNIS/gjOzubdXR0sNPT04D7zjN7s9nMNBoNU6lUrLCwkK2vrzMsCv4Dp/jfhq6HISFhSEgYEhKGhIQhIWFIgk5c47+5/vSe9vb2gHXKysqk+urq6gpYx//a2lfwi5HBQBGGhIQhIWFISBgSEoaEhCEhYUhIGBISFq5MPycnR6peU1OT1CfpMsh8pH9+fg6hhCIMCQlDQsKQkDAkJAwJCUNCwpCQsHAlrjU1NVL1+H0MgeD3s8rw2S1L2FuffgpFGBIShoSEISFhSEgYEhKGhIQhIWFISFi4Mn2TySRV77tvtmEyeNkvIMjeJBMsFGFISBgSEoaEhCEhYUhIGBIShoSEhStx3d/fl6p3cHAQsE5iYqJUX6WlpQHrLCwsQCihCENCwpCQMCQkDAkJQ0LCkJAwJCQMCQlDQv+GAQlFGBIShoSEISFhSEgYEhKGhIQhIWFISBjg+A3HMBlRBy866gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Filtres/Canaux de la 1ière couche de convolution\")\n",
    "displayConvFilters(mnist_model, 'conv1', num_filter=6, filter_size=(5,5), num_channel=0, fig_size=(2,3))\n",
    "print(\"1ier Filtre/Canal de la 2e couche de convolution\")\n",
    "displayConvFilters(mnist_model, 'conv2', num_filter=1, filter_size=(5,5), num_channel=0, fig_size=(1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
